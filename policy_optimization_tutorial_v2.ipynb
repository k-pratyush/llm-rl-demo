{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Optimizing a Linear Policy for MountainCar using an LLM\n",
    "\n",
    "This notebook serves as a detailed guide on utilizing a Large Language Model (LLM), specifically GPT-4o, to perform Policy Search for a linear control policy within an OpenAI Gym reinforcement learning environment. Our focus will be on the MountainCar environment, where we aim to employ GPT-4o to discover the optimal parameters for a simple linear policy that enables the car to successfully reach its goal.\n",
    "\n",
    "##### Environment: State and Action Variables\n",
    "\n",
    "The MountainCar environment presents a classic control challenge framed as a deterministic Markov Decision Process (MDP). In this setup, the environment's future state depends solely on the current state and the action taken, not on the history of preceding states. The term \"deterministic\" signifies that a specific action performed in a particular state will consistently lead to the identical next state and reward. The core task involves an underpowered car, initially placed randomly in the valley between two hills. The objective is to drive the car to the goal located at the top of the right hill. Due to the car's limited engine power, it cannot ascend the steep slope directly. Instead, the agent must learn a strategy of driving back and forth, building momentum to eventually conquer the hill.\n",
    "\n",
    "Control over the car is exerted by applying an external force at each discrete timestep. This force is determined by an action value, $a$, chosen from the continuous range [-1, 1]. This action value is then scaled by a constant factor (0.0015) to yield the actual physical force applied. A positive action propels the car rightward, while a negative action pushes it leftward. At every timestep, the agent observes the environment's current state, which is captured by a two-dimensional vector:\n",
    "$$s = [x, v]$$\n",
    "Here, $x$ represents the car's horizontal position (ranging from -1.2 to 0.6, with the goal at 0.5), and $v$ denotes its current velocity (ranging from -0.07 to 0.07). The environment provides a reward signal at each step; typically, this is -0.1 per step to encourage speed, plus a significant bonus (e.g., +100) upon reaching the goal.\n",
    "\n",
    "##### Policy Representation\n",
    "\n",
    "In reinforcement learning, the agent's behavior is dictated by a \"policy,\" which essentially maps observed states to appropriate actions. For this specific problem, we adopt a straightforward **linear policy**. This implies that the action is calculated as a linear combination of the current state variables (position and velocity). This policy is parameterized by a weight matrix (in this case, a 2x1 vector) denoted as:\n",
    "$$P = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}$$\n",
    "Given a state $s = [x, v]$, the action $a$ is computed through a dot product:\n",
    "$$a = s^T P = x \\cdot w_1 + v \\cdot w_2$$\n",
    "The weights $w_1$ and $w_2$ quantify the influence of the car's position and velocity, respectively, on the chosen action. The central aim of our optimization process is to determine the specific values for $w_1$ and $w_2$ that result in the highest possible total reward accumulated over an entire episode.\n",
    "\n",
    "##### Optimization Strategy: LLM-Driven Policy Search\n",
    "\n",
    "The fundamental objective is to identify the optimal policy parameters $P$ that maximize the cumulative reward $R$ gathered over a complete episode, which consists of a sequence of steps from the start until termination (either reaching the goal or hitting the maximum step limit, e.g., 1000). This task is formally known as **Policy Search**. Mathematically, we seek to solve:\n",
    "$$ \\max_{P} \\mathbb{E}\\left[ \\sum_{t=0}^{T} r_t \\right]$$\n",
    "where $r_t$ is the reward at timestep $t$ and $T$ is the episode length.\n",
    "\n",
    "To facilitate this optimization, we utilize a \"Replay Buffer.\" After each episode concludes, having been run with the current parameters $P$, the total reward $R$ is calculated. This $(P, R)$ pair is then stored in the buffer. We approach the relationship between the policy parameters $P$ and the resulting reward $R$ as a **black-box function**. This means the optimizer, which is the LLM in our case, operates without explicit knowledge of the MountainCar environment's internal physics or reward structure. It only observes the input parameters ($w_1, w_2$) and the corresponding output reward $R$.\n",
    "\n",
    "##### LLM as the Optimizer\n",
    "\n",
    "We harness the capabilities of GPT-4o to conduct this black-box optimization. The LLM is instructed via a prompt to function as an optimization assistant. The process begins with a \"warmup\" phase, where several episodes are run using randomly selected parameters $P$. The resulting $(P, R)$ pairs populate the Replay Buffer, providing initial data. Subsequently, the LLM is presented with a detailed prompt containing the optimization goal, the historical data from the Replay Buffer, output format instructions, and guidance on balancing exploration (trying novel parameters) versus exploitation (refining promising parameters), adapting this balance as the optimization progresses. Based on this prompt and the historical context (enabling in-context learning), the LLM proposes a new set of parameters $P$ anticipated to yield improved rewards.\n",
    "\n",
    "The agent's policy is then updated with these suggested parameters, and one or more evaluation episodes are executed in the environment. The cumulative reward obtained from these evaluations is recorded, and the new $(P, \\text{cumulative } R)$ pair is added to the Replay Buffer. This cycle of prompting the LLM, receiving parameter suggestions, evaluating the updated policy, and updating the buffer is repeated for a predetermined number of episodes (e.g., 400), allowing the LLM to iteratively refine the policy parameters towards optimality. The prompt design treats the task purely as optimizing an unknown function $f(w_1, w_2) = R$, guiding the LLM with hints on step size and search ranges but without revealing the underlying simulation details.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "The implementation follows a modular design. The **World** component (`MountaincarContinuousActionWorld`) wraps the standard Gymnasium environment, managing state transitions, reward calculations, and episode termination. The **Agent** component (`MountaincarContinuousActionLLMNumOptimAgent`) integrates the learning elements. It includes the **`LinearPolicy`** module, which stores the policy weights ($w_1, w_2$) and computes actions based on states. It also contains the **`EpisodeRewardBufferNoBias`** module, responsible for maintaining the Replay Buffer of (weights, reward) pairs. Finally, the **`LLMBrain`** module orchestrates all interactions with the LLM, including prompt generation using Jinja2 templates, API communication (handling both OpenAI and Gemini models), and parsing the LLM's responses to extract the suggested new parameters.\n",
    "\n",
    "##### Hyperparameters\n",
    "\n",
    "Several hyperparameters govern the experiment's execution. `NUM_EPISODES` (e.g., 400) sets the total number of optimization iterations. `RENDER_MODE` controls environment visualization. `MAX_TRAJ_COUNT` (e.g., 1000) defines the Replay Buffer size, influencing the historical context available to the LLM. `MAX_TRAJ_LENGTH` (e.g., 1000) sets the maximum steps per episode. `LLM_MODEL_NAME` specifies the LLM used. `NUM_EVALUATION_EPISODES` (e.g., 20) determines how many runs are averaged to evaluate a new policy. `WARMUP_EPISODES` (e.g., 20) sets the number of initial random runs. `SEARCH_STD` (e.g., 1.0) provides a hint to the LLM regarding the step size for parameter exploration.\n",
    "\n",
    "##### Training Loop\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"./static/training_loop.drawio.svg\" alt=\"image\">\n",
    "</p>\n",
    "\n",
    "\n",
    "The `run_training_loop` function orchestrates the process. It initializes the World and Agent components. It performs the initial warmup runs if necessary, populating the replay buffer. Then, it enters the main loop, iterating `NUM_EPISODES` times. In each iteration, it interacts with the LLM (`agent.train_policy`) to get updated policy parameters based on the replay buffer history. It then evaluates the performance of this new policy over `NUM_EVALUATION_EPISODES` (`agent.evaluate_policy`), calculates the cumulative reward, and adds the new (parameters, cumulative reward) pair back into the replay buffer. Logging occurs at each step.\n",
    "\n",
    "##### Output Structure\n",
    "\n",
    "The training process generates structured logs. A main log directory contains subdirectories for each episode (`episode_*`) and potentially a `warmup/` directory. Each episode directory stores logs of evaluation trajectories, the parameters suggested by the LLM for that episode (`parameters.txt`), and the full LLM interaction including its reasoning (`parameters_reasoning.txt`). The final notebook cells typically include code for visualizing the learned policy in action and plotting the reward curve over episodes, illustrating the learning progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Notebook\n",
    "\n",
    "To run the full experiment:\n",
    "1. Ensure all dependencies are installed (OpenAI API, Google Generative AI, NumPy, Matplotlib, Gymnasium)\n",
    "2. Set your API keys as environment variables\n",
    "   ```bash\n",
    "   export OPENAI_API_KEY=your_key_here\n",
    "   export GEMINI_API_KEY=your_key_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !export OPENAI_API_KEY=\"your_key_here\" # Replace with your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell introduces and lists the key hyperparameters that control the execution and behavior of the reinforcement learning experiment. Hyperparameters are settings that are not learned by the agent itself but are defined by the user before the training process begins. They significantly influence the learning process and the performance of the agent.\n",
    "\n",
    "`NUM_EPISODES` (e.g., 400): Defines the total number of optimization iterations or training episodes the agent will go through. A higher number allows for more learning but increases computation time.\n",
    "\n",
    "`RENDER_MODE` (e.g., None): Controls how the environment is visualized during execution. Options typically include 'human' (real-time window), 'rgb_array' (returns a pixel array, useful for recording), or None (no visualization, fastest for training).\n",
    "\n",
    "`MAX_TRAJ_COUNT` (e.g., 1000): Sets the maximum size of the Replay Buffer. This buffer stores (policy parameters, reward) pairs, and its size determines how much historical data the LLM has access to when making decisions.\n",
    "\n",
    "`MAX_TRAJ_LENGTH` (e.g., 1000): Specifies the maximum number of steps allowed in a single episode. If the agent doesn't reach a terminal state within these steps, the episode is truncated.\n",
    "\n",
    "`LLM_MODEL_NAME` (e.g., \"gemini-2.5-flash-preview-04-17\"): Specifies which Large Language Model will be used as the optimizer. The comment lists several compatible models from OpenAI and Google.\n",
    "NUM_EVALUATION_EPISODES (e.g., 20): Determines how many times a newly proposed policy is run in the environment to get an average measure of its performance. Averaging helps to reduce variance in the reward signal.\n",
    "\n",
    "`WARMUP_EPISODES` (e.g., 20): Sets the number of initial episodes run with randomly generated policy parameters. This \"warmup\" phase populates the Replay Buffer with some initial data points before the LLM starts optimizing.\n",
    "\n",
    "`SEARCH_STD` (e.g., 1.0): Provides a hint to the LLM regarding the standard deviation or step size it should consider when exploring new parameter values, especially during the initial exploration phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES=400 # Total number of episodes to train for\n",
    "RENDER_MODE=None # Choose from 'human', 'rgb_array', or None\n",
    "MAX_TRAJ_COUNT=1000 # Maximum number of trajectories to store in buffer for prompt\n",
    "MAX_TRAJ_LENGTH=1000 # Maximum number of steps in a trajectory\n",
    "LLM_MODEL_NAME=\"gpt-4o\" # LLM for optimization, choose from \"o1-preview\", \"gpt-4o\", \"gemini-2.0-flash-exp\", \"gpt-4o-mini\", \"gemini-1.5-flash\", \"gemini-1.5-flash-8b\", \"gemini-1.5-pro\", \"gemini-2.5-pro-preview-05-06\", \"gemini-2.5-flash-preview-04-17\", \"o3-mini-2025-01-31\", \"gpt-4o-2024-11-20\", \"gpt-4o-2024-08-06\", \"claude-3-7-sonnet-20250219\"\n",
    "\n",
    "NUM_EVALUATION_EPISODES=20 # Number of episodes to generate agent rollouts for evaluation\n",
    "WARMUP_EPISODES=20 # Number of randomly generated initial episodes\n",
    "SEARCH_STD=1.0 # Step size for LLM to search for optimal parameters during exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountaincar_params = {\n",
    "    \"num_episodes\": NUM_EPISODES,\n",
    "    \"gym_env_name\": \"MountainCarContinuous-v0\",\n",
    "    \"render_mode\": RENDER_MODE,\n",
    "    \"logdir\": \"logs/mountaincar_continuous_tutorial\",\n",
    "    \"dim_actions\": 1,\n",
    "    \"dim_states\": 2,\n",
    "    \"max_traj_count\": MAX_TRAJ_COUNT,\n",
    "    \"max_traj_length\": MAX_TRAJ_LENGTH,\n",
    "    \"llm_model_name\": LLM_MODEL_NAME,\n",
    "    \"num_evaluation_episodes\": NUM_EVALUATION_EPISODES,\n",
    "    \"warmup_episodes\": WARMUP_EPISODES,\n",
    "    \"warmup_dir\": None,\n",
    "    \"bias\": True,\n",
    "    \"rank\": None,\n",
    "    \"optimum\": 100,\n",
    "    \"search_step_size\": SEARCH_STD\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell defines the template for the black box optimization prompt. The prompt template uses variables defined in the code for setting the number of parameters required to optimize, the global optimum of the function, step size, current step count and the history of (parameter, reward) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_SI_TEMPLATE_STRING = \"\"\"\n",
    "You are good global optimizer, helping me find the global maximum of a mathematical function f(params).\n",
    "I will give you the function evaluation and the current iteration number at each step. \n",
    "Your goal is to propose input values that efficiently lead us to the global maximum within a limited number of iterations (400). \n",
    "\n",
    "# Regarding the parameters **params**:\n",
    "**params** is an array of {{ rank }} float numbers.\n",
    "**params** values are in the range of [-6.0, 6.0] with 1 decimal place.\n",
    "\n",
    "# Here's how we'll interact:\n",
    "1. I will first provide MAX_STEPS (400) along with a few training examples.\n",
    "2. You will provide your response in the following exact format:\n",
    "    * Line 1: a new input 'params[0]: , params[1]: , params[2]: ,..., params[{{ rank - 1 }}]: ', aiming to maximize the function's value f(params). \n",
    "    Please propose params values in the range of [-6.0, 6.0], with 1 decimal place.\n",
    "    * Line 2: detailed explanation of why you chose that input.\n",
    "3. I will then provide the function's value f(params) at that point, and the current iteration.\n",
    "4. We will repeat steps 2-3 until we reach the maximum number of iterations.\n",
    "\n",
    "# Remember:\n",
    "1. **Do not propose previously seen params.**\n",
    "2. **The global optimum should be around {{ optimum }}.** If you are below that, this is just a local optimum. You should explore instead of exploiting.\n",
    "3. Search both positive and negative values. **During exploration, use search step size of {{ step_size }}**.\n",
    "\n",
    "\n",
    "Next, you will see examples of params and f(params) pairs.\n",
    "{{ episode_reward_buffer_string }}\n",
    "\n",
    "Now you are at iteration {{step_number}} out of 400. Please provide the results in the indicated format. Do not provide any additional texts.\"\"\"\n",
    "\n",
    "\n",
    "llm_si_template = Template(LLM_SI_TEMPLATE_STRING)\n",
    "llm_output_conversion_template = llm_si_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World\n",
    "\n",
    "The `ContinualSpaceGeneralWorld` is a wrapper class over the Gymnasium environments to give standardized interface for the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MountainCarContinuous-v0\n",
    "# https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class ContinualSpaceGeneralWorld():\n",
    "    def __init__(\n",
    "        self,\n",
    "        gym_env_name,\n",
    "        render_mode,\n",
    "        max_traj_length=1000,\n",
    "    ):\n",
    "        assert render_mode in [\"human\", \"rgb_array\", None]\n",
    "\n",
    "        self.env = gym.make(gym_env_name, render_mode=render_mode)\n",
    "        self.gym_env_name = gym_env_name\n",
    "        self.render_mode = render_mode\n",
    "        self.steps = 0\n",
    "        self.accu_reward = 0\n",
    "        self.max_traj_length = max_traj_length\n",
    "        if isinstance(self.env.action_space, gym.spaces.Discrete):\n",
    "            self.discretize = True\n",
    "        else:\n",
    "            self.discretize = False\n",
    "\n",
    "    def reset(self, new_reward=False):\n",
    "        \"\"\" This method resets the environment to its initial state.\n",
    "        If `new_reward` is True, it initializes the environment with a different reward structure.\n",
    "        \"\"\"\n",
    "        del self.env\n",
    "        if not new_reward:\n",
    "            self.env = gym.make(self.gym_env_name, render_mode=self.render_mode)\n",
    "        else:\n",
    "            self.env = gym.make(self.gym_env_name, render_mode=self.render_mode, healthy_reward=0)\n",
    "\n",
    "        state, _ = self.env.reset()\n",
    "        self.steps = 0\n",
    "        self.accu_reward = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        This method executes a step in the environment with the given action.\n",
    "        It updates the environment state, accumulates the reward, and checks if the episode is done.\n",
    "        \"\"\"\n",
    "        self.steps += 1\n",
    "        action = action[0]\n",
    "        state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        self.accu_reward += reward\n",
    "\n",
    "        if self.steps >= self.max_traj_length or terminated or truncated:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        return state, reward, done\n",
    "\n",
    "    def get_accu_reward(self):\n",
    "        \"\"\"\n",
    "        This method returns the accumulated reward for the current episode.\n",
    "        \"\"\"\n",
    "        return self.accu_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Modules\n",
    "\n",
    "`EpisodeRewardBufferNoBias`: Store and manage collection of (policy parameters and reward) pairs, acting as the replay buffer.\n",
    "\n",
    "`LinearPolicy`: Implements a linear policy where the action is computed as a dot product of the state and weights, plus a bias term: $a = s^T W + b$.\n",
    "\n",
    "`LinearPolicyNoBias`: Implements a linear policy without a bias term: $a = s^T W$.\n",
    "\n",
    "`LLMBrain`: Coordinates with the LLM to get new parameters for the policy based on existing policy (parameter, reward) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPolicy():\n",
    "    \"\"\"\n",
    "    Linear policy for continuous action space. The policy is represented as a (2,1) matrix of weights.\n",
    "    Next action is calculated as the dot product of the state and the weight matrix.\n",
    "    state.T * weight + bias -> action\n",
    "    (1,2) * (2,1) + (1,1) -> (1,1)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_states, dim_actions):\n",
    "\n",
    "        self.dim_states =dim_states\n",
    "        self.dim_actions = dim_actions\n",
    "\n",
    "        self.weight = np.random.rand(self.dim_states, self.dim_actions)\n",
    "        self.bias = np.random.rand(1, self.dim_actions)\n",
    "\n",
    "    def initialize_policy(self):\n",
    "        self.weight = np.round(np.random.normal(0., 3., size=(self.dim_states, self.dim_actions)), 1)\n",
    "        self.bias = np.round(np.random.normal(0., 3., size=(1, self.dim_actions)), 1)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = state.T\n",
    "        return np.matmul(state, self.weight) + self.bias\n",
    "\n",
    "    def __str__(self):\n",
    "        output = \"Weights:\\n\"\n",
    "        for w in self.weight:\n",
    "            output += \", \".join([str(i) for i in w])\n",
    "            output += \"\\n\"\n",
    "\n",
    "        output += \"Bias:\\n\"\n",
    "        for b in self.bias:\n",
    "            output += \", \".join([str(i) for i in b])\n",
    "            output += \"\\n\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    def update_policy(self, weight_and_bias_list):\n",
    "        if weight_and_bias_list is None:\n",
    "            return\n",
    "\n",
    "        weight_and_bias_list = np.array(weight_and_bias_list).reshape(self.dim_states + 1, self.dim_actions)\n",
    "        self.weight = np.array(weight_and_bias_list[:-1])\n",
    "        self.bias = np.expand_dims(np.array(weight_and_bias_list[-1]), axis=0)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        parameters = np.concatenate((self.weight, self.bias), axis=0)\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "The below cell defines the core agent wrapper. It is responsibe for managing the policy, interacting with the world and coordinating with the LLMBrain to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMNumOptimAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        logdir,\n",
    "        dim_action,\n",
    "        dim_state,\n",
    "        max_traj_count,\n",
    "        max_traj_length,\n",
    "        llm_si_template,\n",
    "        llm_output_conversion_template,\n",
    "        llm_model_name,\n",
    "        num_evaluation_episodes,\n",
    "        bias,\n",
    "        optimum,\n",
    "        search_step_size,\n",
    "    ):\n",
    "        self.start_time = time.process_time()\n",
    "        self.api_call_time = 0\n",
    "        self.total_steps = 0\n",
    "        self.total_episodes = 0\n",
    "        self.dim_action = dim_action\n",
    "        self.dim_state = dim_state\n",
    "        self.bias = bias\n",
    "        self.optimum = optimum\n",
    "        self.search_step_size = search_step_size\n",
    "\n",
    "        if not self.bias:\n",
    "            param_count = dim_action * dim_state\n",
    "        else:\n",
    "            param_count = dim_action * dim_state + dim_action\n",
    "        self.rank = param_count\n",
    "\n",
    "        # Initialize the policy and replay buffer\n",
    "        if not self.bias:\n",
    "            self.policy = LinearPolicyNoBias(\n",
    "                dim_actions=dim_action, dim_states=dim_state\n",
    "            )\n",
    "        else:\n",
    "            self.policy = LinearPolicy(dim_actions=dim_action, dim_states=dim_state)\n",
    "        self.replay_buffer = EpisodeRewardBufferNoBias(max_size=max_traj_count)\n",
    "        self.llm_brain = LLMBrain(\n",
    "            llm_si_template, llm_output_conversion_template, llm_model_name\n",
    "        )\n",
    "        self.logdir = logdir\n",
    "        self.num_evaluation_episodes = num_evaluation_episodes\n",
    "        self.training_episodes = 0\n",
    "\n",
    "        if self.bias:\n",
    "            self.dim_state += 1\n",
    "\n",
    "    def rollout_episode(self, world, logging_file, record=True):\n",
    "        \"\"\"Simulates an episode in the environment using the current policy.\"\"\"\n",
    "        state = world.reset()\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        logging_file.write(\n",
    "            f\"{', '.join([str(x) for x in self.policy.get_parameters().reshape(-1)])}\\n\"\n",
    "        )\n",
    "        logging_file.write(f\"parameter ends\\n\\n\")\n",
    "        logging_file.write(f\"state | action | reward\\n\")\n",
    "        done = False\n",
    "        step_idx = 0\n",
    "        while not done:\n",
    "            action = self.policy.get_action(state.T)\n",
    "            action = np.reshape(action, (1, self.dim_action))\n",
    "            if world.discretize:\n",
    "                action = np.argmax(action)\n",
    "                action = np.array([action])\n",
    "            next_state, reward, done = world.step(action)\n",
    "            logging_file.write(f\"{state.T[0]} | {action[0]} | {reward}\\n\")\n",
    "            state = next_state\n",
    "            step_idx += 1\n",
    "            self.total_steps += 1\n",
    "        logging_file.write(f\"Total reward: {world.get_accu_reward()}\\n\")\n",
    "        self.total_episodes += 1\n",
    "        if record:\n",
    "            self.replay_buffer.add(\n",
    "                self.policy.get_parameters(), world.get_accu_reward()\n",
    "            )\n",
    "        return world.get_accu_reward()\n",
    "\n",
    "    def random_warmup(self, world, logdir, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            self.policy.initialize_policy()\n",
    "            # Run the episode and collect the trajectory\n",
    "            print(f\"Rolling out warmup episode {episode}...\")\n",
    "            logging_filename = f\"{logdir}/warmup_rollout_{episode}.txt\"\n",
    "            logging_file = open(logging_filename, \"w\")\n",
    "            result = self.rollout_episode(world, logging_file)\n",
    "            print(f\"Result: {result}\")\n",
    "\n",
    "    def train_policy(self, world, logdir):\n",
    "        \"\"\"Core method to train single iteration of the policy using LLM optimization.\"\"\"\n",
    "\n",
    "        def parse_parameters(input_text):\n",
    "            # This regex looks for integers or floating-point numbers (including optional sign)\n",
    "            s = input_text.split(\"\\n\")[0]\n",
    "            print(\"response:\", s)\n",
    "            pattern = re.compile(r\"params\\[(\\d+)\\]:\\s*([+-]?\\d+(?:\\.\\d+)?)\")\n",
    "            matches = pattern.findall(s)\n",
    "\n",
    "            # Convert matched strings to float (or int if you prefer to differentiate)\n",
    "            results = []\n",
    "            for match in matches:\n",
    "                results.append(float(match[1]))\n",
    "            print(results)\n",
    "            assert len(results) == self.rank\n",
    "            return np.array(results).reshape(-1)\n",
    "\n",
    "        def str_nd_examples(replay_buffer: EpisodeRewardBufferNoBias, n):\n",
    "\n",
    "            all_parameters = []\n",
    "            for weights, reward in replay_buffer.buffer:\n",
    "                parameters = weights\n",
    "                all_parameters.append((parameters.reshape(-1), reward))\n",
    "\n",
    "            text = \"\"\n",
    "            for parameters, reward in all_parameters:\n",
    "                l = \"\"\n",
    "                for i in range(n):\n",
    "                    l += f\"params[{i}]: {parameters[i]:.5g}; \"\n",
    "                fxy = reward\n",
    "                l += f\"f(params): {fxy:.2f}\\n\"\n",
    "                text += l\n",
    "            return text\n",
    "\n",
    "        # Update the policy using llm_brain, q_table and replay_buffer\n",
    "        print(\"Updating the policy...\")\n",
    "        new_parameter_list, reasoning, api_time = self.llm_brain.llm_update_parameters_num_optim(\n",
    "            str_nd_examples(self.replay_buffer, self.rank),\n",
    "            parse_parameters,\n",
    "            self.training_episodes,\n",
    "            self.rank,\n",
    "            self.optimum,\n",
    "            self.search_step_size\n",
    "        )\n",
    "        self.api_call_time += api_time\n",
    "\n",
    "        print(self.policy.get_parameters().shape)\n",
    "        print(new_parameter_list.shape)\n",
    "\n",
    "        self.policy.update_policy(new_parameter_list)\n",
    "        \n",
    "        print(self.policy.get_parameters().shape)\n",
    "        logging_q_filename = f\"{logdir}/parameters.txt\"\n",
    "        logging_q_file = open(logging_q_filename, \"w\")\n",
    "        logging_q_file.write(str(self.policy))\n",
    "        logging_q_file.close()\n",
    "        \n",
    "        q_reasoning_filename = f\"{logdir}/parameters_reasoning.txt\"\n",
    "        q_reasoning_file = open(q_reasoning_filename, \"w\")\n",
    "        q_reasoning_file.write(reasoning)\n",
    "        q_reasoning_file.close()\n",
    "        print(\"Policy updated!\")\n",
    "\n",
    "        # Run the episode and collect the trajectory\n",
    "        print(f\"Rolling out episode {self.training_episodes}...\")\n",
    "        logging_filename = f\"{logdir}/training_rollout.txt\"\n",
    "        logging_file = open(logging_filename, \"w\")\n",
    "        results = []\n",
    "        for idx in range(self.num_evaluation_episodes):\n",
    "            if idx == 0:\n",
    "                result = self.rollout_episode(world, logging_file, record=False)\n",
    "            else:\n",
    "                result = self.rollout_episode(world, logging_file, record=False)\n",
    "            results.append(result)\n",
    "        print(f\"Results: {results}\")\n",
    "        result = np.mean(results)\n",
    "        self.replay_buffer.add(new_parameter_list, result)\n",
    "\n",
    "        self.training_episodes += 1\n",
    "\n",
    "        _cpu_time = time.process_time() - self.start_time\n",
    "        _api_time = self.api_call_time\n",
    "        _total_episodes = self.total_episodes\n",
    "        _total_steps = self.total_steps\n",
    "        _total_reward = result\n",
    "        return _cpu_time, _api_time, _total_episodes, _total_steps, _total_reward\n",
    "    \n",
    "\n",
    "    def evaluate_policy(self, world, logdir):\n",
    "        results = []\n",
    "        for idx in range(self.num_evaluation_episodes):\n",
    "            logging_filename = f\"{logdir}/evaluation_rollout_{idx}.txt\"\n",
    "            logging_file = open(logging_filename, \"w\")\n",
    "            result = self.rollout_episode(world, logging_file, record=False)\n",
    "            results.append(result)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell orchestrates the entire training process from initialization to completion. The `run_training_loop` function starts with initialization the world, and the agent instances. Then, it creates a set of warmup episodes to pass in as initial replay buffer to the optimizer. The code then runs the training loop for specified number of episodes and optimizes the policy parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_loop(\n",
    "    num_episodes,\n",
    "    gym_env_name,\n",
    "    render_mode,\n",
    "    logdir,\n",
    "    dim_actions,\n",
    "    dim_states,\n",
    "    max_traj_count,\n",
    "    max_traj_length,\n",
    "    llm_model_name,\n",
    "    num_evaluation_episodes,\n",
    "    warmup_episodes,\n",
    "    warmup_dir,\n",
    "    bias=None,\n",
    "    rank=None,\n",
    "    optimum=100,\n",
    "    search_step_size=SEARCH_STD,\n",
    "):\n",
    "    world = ContinualSpaceGeneralWorld(\n",
    "        gym_env_name,\n",
    "        render_mode,\n",
    "        max_traj_length,\n",
    "    )\n",
    "\n",
    "    agent = LLMNumOptimAgent(\n",
    "        logdir,\n",
    "        dim_actions,\n",
    "        dim_states,\n",
    "        max_traj_count,\n",
    "        max_traj_length,\n",
    "        llm_si_template,\n",
    "        llm_output_conversion_template,\n",
    "        llm_model_name,\n",
    "        num_evaluation_episodes,\n",
    "        bias,\n",
    "        optimum,\n",
    "        search_step_size,\n",
    "    )\n",
    "    print('init done')\n",
    "\n",
    "    if not warmup_dir:\n",
    "        warmup_dir = f\"{logdir}/warmup\"\n",
    "        os.makedirs(warmup_dir, exist_ok=True)\n",
    "        agent.random_warmup(world, warmup_dir, warmup_episodes)\n",
    "    else:\n",
    "        agent.replay_buffer.load(warmup_dir)\n",
    "    \n",
    "    overall_log_file = open(f\"{logdir}/overall_log.txt\", \"w\")\n",
    "    overall_log_file.write(\"Iteration, CPU Time, API Time, Total Episodes, Total Steps, Total Reward\\n\")\n",
    "    overall_log_file.flush()\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Episode: {episode}\")\n",
    "        # create log dir\n",
    "        curr_episode_dir = f\"{logdir}/episode_{episode}\"\n",
    "        print(f\"Creating log directory: {curr_episode_dir}\")\n",
    "        os.makedirs(curr_episode_dir, exist_ok=True)\n",
    "        \n",
    "        for trial_idx in range(5):\n",
    "            try:\n",
    "                cpu_time, api_time, total_episodes, total_steps, total_reward = agent.train_policy(world, curr_episode_dir)\n",
    "                overall_log_file.write(f\"{episode + 1}, {cpu_time}, {api_time}, {total_episodes}, {total_steps}, {total_reward}\\n\")\n",
    "                overall_log_file.flush()\n",
    "                print(f\"{trial_idx + 1}th trial attempt succeeded in training\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"{trial_idx + 1}th trial attempt failed with error in training: {e}\"\n",
    "                )\n",
    "                traceback.print_exc()\n",
    "\n",
    "                if trial_idx == 4:\n",
    "                    print(f\"All {trial_idx + 1} trials failed. Train terminated\")\n",
    "                    exit(1)\n",
    "                continue\n",
    "    overall_log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_loop(**mountaincar_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy(*args, **kwargs):\n",
    "    agent = LLMNumOptimAgent(\n",
    "        kwargs['logdir'],\n",
    "        dim_action=kwargs['dim_actions'],\n",
    "        dim_state=kwargs['dim_states'],\n",
    "        max_traj_count=kwargs['max_traj_count'],\n",
    "        max_traj_length=kwargs['max_traj_length'],\n",
    "        llm_si_template=llm_si_template,\n",
    "        llm_output_conversion_template=llm_output_conversion_template,\n",
    "        llm_model_name=kwargs['llm_model_name'],\n",
    "        num_evaluation_episodes=kwargs['num_evaluation_episodes'],\n",
    "        bias=kwargs['bias'],\n",
    "        optimum=kwargs['optimum'],\n",
    "        search_step_size=SEARCH_STD\n",
    "    )\n",
    "\n",
    "    world = ContinualSpaceGeneralWorld(\n",
    "        kwargs['gym_env_name'],\n",
    "        render_mode=\"rgb_array\",\n",
    "        max_traj_length=kwargs['max_traj_length']\n",
    "    )\n",
    "\n",
    "    parameter_filename = os.path.join(kwargs['logdir'], kwargs['episode_dir'], \"parameters.txt\")\n",
    "    parameters = []\n",
    "    with open(parameter_filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if \"parameter ends\" in line:\n",
    "                break\n",
    "            try:\n",
    "                parameters.append([float(x) for x in line.split(\",\")])\n",
    "            except:\n",
    "                continue\n",
    "        parameters = np.array(parameters)\n",
    "\n",
    "    agent.policy.update_policy(parameters)    \n",
    "    state = world.reset()\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "\n",
    "    done = False\n",
    "    step_idx = 0\n",
    "    frames = []  # List to store frames for GIF generation\n",
    "\n",
    "    while not done:\n",
    "        img = world.env.render()\n",
    "        if isinstance(img, np.ndarray):\n",
    "            frames.append(img)  # Append rendered frame to the list\n",
    "        else:\n",
    "            img = np.array(img)\n",
    "            frames.append(img)\n",
    "\n",
    "        action = agent.policy.get_action(state.T)\n",
    "        action = np.reshape(action, (1, kwargs['dim_actions']))\n",
    "        next_state, reward, done = world.step(action)\n",
    "        state = next_state\n",
    "        step_idx += 1\n",
    "\n",
    "    # Save frames as a GIF\n",
    "    gif_filename = os.path.join(kwargs['logdir'], kwargs['episode_dir'], \"policy_visualization.gif\")\n",
    "    imageio.mimsave(gif_filename, frames, fps=30)\n",
    "\n",
    "    return gif_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "episode_399_gif = run_policy(**mountaincar_params, episode_dir=\"episode_399\")\n",
    "\n",
    "display(HTML(\"<h3>Episode 399</h3>\"))\n",
    "display(HTML(f'<img src=\"{episode_399_gif}\" style=\"width: 100%; max-width: 800px;\">'))\n",
    "\n",
    "cpu_times, api_times, total_episodes, total_steps, total_rewards = read_file(find_file(mountaincar_params['logdir']))\n",
    "plot_data(total_episodes, total_rewards, \"ProPS Optimization on MountainCar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Extension: Optimizing a Linear Policy for Swimmer-v5 using an LLM\n",
    "\n",
    "This section extends the tutorial by applying the LLM-driven optimization approach to a more complex task: the Swimmer-v5 environment from the Mujoco physics simulation suite, available through Gymnasium. Similar to the MountainCar problem, our goal is to utilize a Large Language Model (LLM), such as GPT-4o or a Gemini model, to perform Policy Search. We will aim to discover the optimal parameters for a linear policy that enables the swimmer agent to achieve efficient forward locomotion.\n",
    "\n",
    "##### Environment: State and Action Variables\n",
    "\n",
    "The Swimmer-v5 environment presents a challenging continuous control problem. The agent is a simple swimmer composed of three rigid links connected by two actuated rotational joints (rotors). This chain-like structure is simulated in a viscous fluid. The primary objective for the swimmer is to move forward (typically along the positive x-axis) as quickly as possible by applying torques to its two rotors. The interaction with the fluid and the multi-link dynamics make this a non-trivial control task. Like MountainCar, this environment can be modeled as a Markov Decision Process (MDP), where the next state and reward are determined by the current state and the action taken. Mujoco environments are generally deterministic given the same initial conditions and actions.\n",
    "\n",
    "At each timestep, the agent receives an observation of the environment's current state. For Swimmer-v5, this state is represented by an 8-dimensional continuous vector:\n",
    "$$ S = [q_{tip}, q_{rotor1}, q_{rotor2}, v_x, v_y, \\omega_{tip}, \\omega_{rotor1}, \\omega_{rotor2} ] $$\n",
    "Where:\n",
    "*   $q_{tip}$: Angle of the front tip (the first link).\n",
    "*   $q_{rotor1}$: Angle of the first rotor.\n",
    "*   $q_{rotor2}$: Angle of the second rotor.\n",
    "*   $v_x$: Velocity of the tip along the x-axis (forward direction).\n",
    "*   $v_y$: Velocity of the tip along the y-axis.\n",
    "*   $\\omega_{tip}$: Angular velocity of the front tip.\n",
    "*   $\\omega_{rotor1}$: Angular velocity of the first rotor.\n",
    "*   $\\omega_{rotor2}$: Angular velocity of the second rotor.\n",
    "\n",
    "Control over the swimmer is exerted by applying torques to its two rotors. The action $A$ is a 2-dimensional continuous vector:\n",
    "$$ A = [ \\tau_1, \\tau_2 ] $$\n",
    "Here, $\\tau_1$ is the torque applied to the first rotor, and $\\tau_2$ is the torque applied to the second rotor. Both torque values are typically clipped within the range [-1, 1]. The reward function in Swimmer-v5 is primarily based on the forward velocity ($v_x$), encouraging the agent to swim quickly in the target direction. There might also be small control costs to discourage excessive torque usage.\n",
    "\n",
    "##### Policy Representation\n",
    "\n",
    "Consistent with the approach for MountainCar, we will employ a **linear policy** for the Swimmer. The action (torques) will be a linear combination of the observed state variables. Given the 8 state variables and 2 action variables, the policy will be parameterized by a weight matrix $W$ of shape 8x2:\n",
    "$$ W = \\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} \\\\\n",
    "w_{2,1} & w_{2,2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "w_{8,1} & w_{8,2}\n",
    "\\end{bmatrix} $$\n",
    "Given a state vector $S$ (an 8x1 vector), the action vector $A$ (a 2x1 vector) is computed as:\n",
    "$$ A = S^T W $$\n",
    "Each element $A_j = \\sum_{i=1}^{8} S_i \\cdot w_{i,j}$. The weights $w_{i,j}$ determine the influence of the $i$-th state variable on the $j$-th action (torque). The optimization goal is to find the 16 weights in $W$ that maximize the total reward accumulated over an episode.\n",
    "\n",
    "##### Optimization Strategy: LLM-Driven Policy Search\n",
    "\n",
    "The core objective remains to find the optimal policy parameters $W$ that maximize the expected cumulative reward $R$ over an episode. This is a Policy Search problem:\n",
    "$$ \\max_{W} \\mathbb{E}\\left[ \\sum_{t=0}^{T} r_t \\right]$$\n",
    "We will again use a Replay Buffer to store pairs of (policy parameters $W$, total reward $R$). The LLM will treat the relationship $f(W) = R$ as a **black-box function**, learning to propose better parameters based on observed performance without direct knowledge of the Swimmer's physics or complex dynamics.\n",
    "\n",
    "##### LLM as the Optimizer\n",
    "\n",
    "The chosen LLM will act as the optimization engine. The process mirrors that used for MountainCar:\n",
    "1.  **Warmup Phase**: Initial episodes are run with randomly generated policy parameters $W$ to populate the Replay Buffer.\n",
    "2.  **Iterative Refinement**: The LLM is prompted with the optimization goal, the historical data from the Replay Buffer (pairs of $W$ and $R$), output format instructions, and guidance on exploration vs. exploitation.\n",
    "3.  The LLM proposes a new set of parameters $W'$.\n",
    "4.  The agent's policy is updated with $W'$, and evaluation episodes are run.\n",
    "5.  The resulting ( $W'$, cumulative $R'$) pair is added to the Replay Buffer.\n",
    "This cycle repeats, allowing the LLM to iteratively refine the 16 policy parameters for the Swimmer agent. The prompt will guide the LLM to optimize this higher-dimensional function $f(W) = R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmer_params = {\n",
    "    \"num_episodes\": NUM_EPISODES,\n",
    "    \"gym_env_name\": \"Swimmer-v5\",\n",
    "    \"render_mode\": RENDER_MODE,\n",
    "    \"logdir\": \"logs/mujoco_swimmer_tutorial\",\n",
    "    \"dim_actions\": 2,\n",
    "    \"dim_states\": 8,\n",
    "    \"max_traj_count\": MAX_TRAJ_COUNT,\n",
    "    \"max_traj_length\": MAX_TRAJ_LENGTH,\n",
    "    \"llm_model_name\": LLM_MODEL_NAME,\n",
    "    \"num_evaluation_episodes\": NUM_EVALUATION_EPISODES,\n",
    "    \"warmup_episodes\": WARMUP_EPISODES,\n",
    "    \"warmup_dir\": None,\n",
    "    \"bias\": True,\n",
    "    \"rank\": None,\n",
    "    \"optimum\": 250,\n",
    "    \"search_step_size\": SEARCH_STD\n",
    "}\n",
    "\n",
    "run_training_loop(**swimmer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmer_episode_399_gif = run_policy(**swimmer_params, episode_dir=\"episode_399\")\n",
    "\n",
    "display(HTML(\"<h3>Swimmer Episode 399</h3>\"))\n",
    "display(HTML(f'<img src=\"{swimmer_episode_399_gif}\" style=\"width: 100%; max-width: 800px;\">'))\n",
    "\n",
    "cpu_times, api_times, total_episodes, total_steps, total_rewards = read_file(find_file(swimmer_params['logdir']))\n",
    "plot_data(total_episodes, total_rewards, \"ProPS Optimization on Swimmer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
